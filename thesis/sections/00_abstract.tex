\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This thesis investigates clinically interpretable explainable artificial intelligence for chest
radiograph diagnosis, with emphasis on explanations that are faithful to model behavior rather than
only plausible to human readers. The work proposes a unified evaluation framework that compares
two primary explanation families under a common protocol: post-hoc saliency maps and concept-based
explanations. Using a shared chest X-ray benchmark design, the study evaluates both diagnostic
performance and explanation quality with standardized faithfulness tests, including randomization
sanity checks, perturbation-based deletion and insertion analyses, and robustness assessments. The
current implemented contribution is a reproducible benchmark pipeline covering cohort audit,
baseline modeling, saliency benchmarking, concept-family artifact integration, unified cross-method
scoring, and randomization-sanity aggregation. Text rationale methods remain explicitly out of core
scope in this master's phase and are framed as a future extension after the saliency-versus-concept
benchmark is fully validated.

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This thesis investigates clinically interpretable explainable artificial intelligence for chest radiograph diagnosis, with emphasis on explanations that are faithful to model behavior rather than only plausible to human readers. The work proposes a unified evaluation framework that compares three explanation families under a common protocol: post-hoc saliency maps, concept-based explanations, and text-based rationales. Using a shared chest X-ray benchmark design, the study evaluates both diagnostic performance and explanation quality with standardized faithfulness tests, including randomization sanity checks, perturbation-based deletion and insertion analyses, and robustness assessments. A central component is the comparison of constrained, concept-grounded text rationales against less-constrained text generation to quantify the trade-off between fluency and faithfulness. The expected contribution is a reproducible benchmark pipeline and an empirical analysis of how explanation mechanisms behave under controlled interventions, providing clearer guidance for developing safer and more interpretable radiology AI systems in research settings.

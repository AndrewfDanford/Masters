\chapter{Experimental Design}

\section{Datasets and Splits}
The primary experiments use MIMIC-CXR for diagnostic modeling \citep{johnson2019mimiccxr}. MS-CXR
supports localization-oriented evaluation \citep{boecking2024mscxr}, and RadGraph resources support
concept extraction \citep{jain2021radgraph}.

\subsection{Cohort Construction}
\begin{itemize}
  \item Keep frontal views (AP/PA) only.
  \item Apply patient-level splitting to avoid leakage.
  \item Restrict to six core findings: Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion, Pneumothorax.
  \item Exclude records with missing core metadata required for split reproducibility.
\end{itemize}

\subsection{Primary and Secondary Evaluation Partitions}
\begin{itemize}
  \item \textbf{Primary partition:} in-domain test split for all metrics.
  \item \textbf{Stress partition:} perturbation and confound-focused subsets.
  \item \textbf{Localization partition:} MS-CXR-linked subset for region metrics.
\end{itemize}

\section{Experiment Matrix}
Table~\ref{tab:experiment-matrix} defines the core experiment set.

\begin{table}[ht]
\centering
\small
\begin{tabular}{p{0.12\linewidth}p{0.36\linewidth}p{0.42\linewidth}}
\toprule
\textbf{ID} & \textbf{System} & \textbf{Goal} \\
\midrule
E1 & Baseline CXR classifier & Establish predictive reference (AUROC/AUPRC/calibration). \\
E2 & E1 + Grad-CAM & Saliency baseline under unified faithfulness tests. \\
E3 & E1 + HiResCAM & Alternative saliency baseline with robustness comparison. \\
E4 & Concept-head / CBM-style model & Evaluate concept-grounded faithfulness and intervention behavior. \\
E7 & Unified faithfulness run (E2--E4) & Main thesis comparison via shared protocol and NFI. \\
E8 & Sanity-check suite (randomization) & Verify explanation sensitivity to model/data integrity. \\
E9 & Spurious-cue stress test (optional extension) & Evaluate robustness to plausible confounding conditions. \\
\bottomrule
\end{tabular}
\caption{Core experiments through Section 6 scope.}
\label{tab:experiment-matrix}
\end{table}

\section{Implementation Status at Draft Time}
At the time of this draft, the executable pipeline is complete for E0, E1, E2--E3, E7, and E8 in an
artifact-driven form. Concretely, the repository includes:
\begin{itemize}
  \item cohort construction and prevalence audit tooling (E0),
  \item baseline training from feature tables and end-to-end image training (E1),
  \item saliency artifact generation and scoring for Grad-CAM/HiResCAM (E2--E3),
  \item concept-family artifact generation with frozen contracts for model-backed E4 replacement,
  \item unified cross-family scoring with confidence intervals and quality gates (E7),
  \item multi-run randomization sanity aggregation with pass-rate reporting (E8).
\end{itemize}
E4 model training is not yet implemented in this draft, but artifact schemas and method contracts
are frozen to prevent interface drift during implementation. Text rationale tracks (former E5/E6)
are intentionally deferred to future work. E9 remains pending as a dedicated stress-test
data-builder and evaluation track.

\section{Primary Metrics}
Metrics are reported by split and by finding, then aggregated with confidence intervals.

\subsection{Diagnostic Metrics}
\begin{itemize}
  \item AUROC and AUPRC (macro and per-label).
  \item Calibration metrics (ECE and Brier score).
\end{itemize}

\subsection{Explanation Metrics}
\begin{itemize}
  \item \textbf{Sanity checks (all families):} degradation under model and label randomization.
  \item \textbf{Deletion/insertion (all families):}
  confidence curves under evidence removal/recovery.
  \item \textbf{Nuisance robustness (all families):} explanation stability under mild
  perturbations, evaluated only when predictions remain stable.
  \item \textbf{Family-specific secondary metrics:}
  saliency localization agreement and concept intervention coherence.
\end{itemize}

\subsection{Primary Explanation Endpoints}
Primary explanation endpoints are the three component metrics:
\begin{itemize}
  \item sanity score,
  \item perturbation score (from deletion/insertion behavior),
  \item nuisance robustness score.
\end{itemize}
All three are reported with uncertainty intervals and interpreted directly before any aggregate
index is considered.

\subsection{Secondary Aggregate Endpoint}
NFI (Section 5) is used as a secondary summary index for compact comparison and ranking. Any claim
based on NFI must remain consistent with the component-level evidence.

\section{Ablation and Sensitivity Studies}
To ensure conclusions are not artifact-driven:
\begin{enumerate}
  \item \textbf{Concept supervision fraction:} evaluate reduced concept annotation settings (for
  example 25\%, 50\%, 100\%).
  \item \textbf{Perturbation granularity:} coarse versus fine evidence masks.
  \item \textbf{Backbone sensitivity:} repeat core experiments with one alternate classifier backbone.
  \item \textbf{Uncertain-label policy sensitivity:} compare primary policy with at least one
  alternative mapping.
\end{enumerate}

\section{Statistical Analysis Plan}
All cross-method comparisons are paired on identical study IDs.
\begin{itemize}
  \item Confidence intervals use study-level bootstrap resampling.
  \item Pairwise method differences are reported with effect sizes and confidence intervals.
  \item Multiple-comparison correction is applied within each metric family.
\end{itemize}

\subsection{Precision Targets}
To prevent over-interpretation of noisy differences, conclusions are treated as confirmatory only
when interval precision is adequate. As operational targets:
\begin{itemize}
  \item diagnostic delta-AUROC intervals should be sufficiently narrow to distinguish practically relevant differences,
  \item component-level faithfulness differences (sanity, perturbation, robustness) should remain directionally stable across bootstrap runs and sensitivity analyses.
  \item secondary NFI ranking should not contradict component-level conclusions.
\end{itemize}
If precision targets are not met, findings are reported as exploratory.

\subsection{Analysis Governance}
Primary endpoints, model selection rules, intervention bounds, and ablation order are fixed before
final test evaluation. Any post hoc analysis is labeled explicitly to separate confirmatory from
exploratory evidence.

\section{Failure Analysis and Quality Gates}
Beyond aggregate scores, each run includes structured failure analysis:
\begin{itemize}
  \item errors stratified by pathology prevalence and confidence bins,
  \item disagreement cases between explanation families,
  \item examples where explanations remain plausible but fail perturbation criteria.
\end{itemize}

Two quality gates must be met before claiming a stable conclusion:
\begin{enumerate}
  \item At least two explanation families pass sanity checks.
  \item Component-level method ordering is robust across at least one sensitivity setting, with NFI used only as a secondary consistency check.
\end{enumerate}

\section{Reproducibility Deliverables}
Each experiment release includes:
\begin{itemize}
  \item frozen split manifests and label maps,
  \item config files for training and evaluation,
  \item metric tables with confidence intervals,
  \item selected qualitative examples linked to quantitative failures.
\end{itemize}
